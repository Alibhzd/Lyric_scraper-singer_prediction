{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85199848",
   "metadata": {},
   "source": [
    "# Lyric scraper and singer prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf06dc7",
   "metadata": {},
   "source": [
    "# Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a484527b",
   "metadata": {},
   "source": [
    "In this project, i have used web scraping to get the lyrics for two singers and at the end i have used a machine learning algorithm to predict the singer of a given lyric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321fc74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3e522",
   "metadata": {},
   "source": [
    "# Singers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e3c0d",
   "metadata": {},
   "source": [
    "## Taylor Swift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_Tswift = 'https://www.lyrics.com/artist/Taylor-Swift/816977'\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/110.0\"}\n",
    "\n",
    "page = requests.get(URL_Tswift, headers = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39db5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3ea526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the html text data\n",
    "page_html = page.text\n",
    "#print(page_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d73b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1 = BeautifulSoup(page_html, 'html.parser')\n",
    "soup2 = BeautifulSoup(soup1.prettify(), 'html.parser')\n",
    "#print(soup2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6512103",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs= soup2.find_all(name='td', attrs={'class':'tal qx'})\n",
    "#print(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd95d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(songs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9fc868",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4869ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url_list = list()\n",
    "\n",
    "\n",
    "for index, element in enumerate(songs):# use enumerate to get the index \n",
    "    a_tag = element.find(name=\"a\")\n",
    "    title = a_tag.get_text()\n",
    "    url_song = a_tag['href']\n",
    "    print(\"Index:\", index)\n",
    "    print(\"Names:\", title)\n",
    "    full_song_url = \"https://www.lyrics.com\" + url_song\n",
    "    print(full_song_url)\n",
    "    response = requests.get(full_song_url, headers=headers)\n",
    "    html_current_url = response.text\n",
    "    with open(title, 'w', encoding='UTF8') as file:\n",
    "        file.write(html_current_url)\n",
    "    time.sleep(1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "lyrics_data = []\n",
    "\n",
    "filedir = \"./Lyrics/TaylorSwift/\"\n",
    "\n",
    "for filename in os.listdir(filedir):\n",
    "    file_path = filedir + filename\n",
    "    with open(file_path, 'r', encoding='UTF8') as file:\n",
    "        html_content = file.read()\n",
    "    singer = 'TaylorSwift'    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    lyrics_tags = soup.find_all('pre', {'id':'lyric-body-text'})\n",
    "    for lyrics_tag in lyrics_tags:\n",
    "        lyrics = lyrics_tag.get_text()\n",
    "        lyrics_data.append({'Singer': singer, 'Lyrics': lyrics})\n",
    "        \n",
    "df_TSwift = pd.DataFrame(lyrics_data)\n",
    "print(df_TSwift)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_TSwift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fa38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TSwift.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac47034",
   "metadata": {},
   "source": [
    "# Adele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddfad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_Adele = 'https://www.lyrics.com/artist/Adele/861756'\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/110.0\"}\n",
    "\n",
    "page = requests.get(URL_Adele, headers = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b8936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the html text data\n",
    "page_html = page.text\n",
    "#print(page_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4642e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1 = BeautifulSoup(page_html, 'html.parser')\n",
    "soup2 = BeautifulSoup(soup1.prettify(), 'html.parser')\n",
    "#print(soup2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d206b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs= soup2.find_all(name='td', attrs={'class':'tal qx'})\n",
    "#print(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d752046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = \"./Lyrics/Adele/\"\n",
    "\n",
    "for index, element in enumerate(songs):# use enumerate to get the index \n",
    "    #print(index, element)\n",
    "    a_tag = element.find(name=\"a\")\n",
    "    title = a_tag.get_text()\n",
    "    url_song = a_tag['href']\n",
    "    print(\"Index:\", index)\n",
    "    print(\"Names:\", title)\n",
    "    full_song_url = \"https://www.lyrics.com\" + url_song\n",
    "    print(full_song_url)\n",
    "    response = requests.get(full_song_url, headers=headers)\n",
    "    html_current_url = response.text\n",
    "    file_path = folder_path + f'{index}_{title}'\n",
    "    with open(file_path, 'w', encoding='UTF8') as file:\n",
    "        file.write(html_current_url)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c3317",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lyrics_data = []\n",
    "\n",
    "filedir = \"./Lyrics/Adele/\"\n",
    "\n",
    "for filename in os.listdir(filedir):\n",
    "    file_path = filedir + filename\n",
    "    with open(file_path, 'r', encoding='UTF8') as file:\n",
    "        html_content = file.read()\n",
    "    singer = 'Adele'    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    lyrics_tags = soup.find_all('pre', {'id':'lyric-body-text'})\n",
    "    for lyrics_tag in lyrics_tags:\n",
    "        lyrics = lyrics_tag.get_text()\n",
    "        lyrics_data.append({'Singer': singer, 'Lyrics': lyrics})\n",
    "        \n",
    "        \n",
    "df_Adele = pd.DataFrame(lyrics_data)\n",
    "print(df_Adele)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bfd7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lyrics_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9045742d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_Adele.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda7344",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df_TSwift, df_Adele], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903fbff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3517f70d",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dae2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as skl_stopwords\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer= TreebankWordTokenizer()\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b242b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "corpus = combined_df['Lyrics']#.tolist()\n",
    "\n",
    "#labels\n",
    "labels = combined_df['Singer']#.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81833ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a custom function to strip punctuation, tokenize and lemmatize as well as remove stopwords\n",
    "def tokenize_lemmatize(text, stopwords=skl_stopwords, tokenizer=tokenizer, lemmatizer=lemmatizer):\n",
    "    text = ''.join([ch for ch in text if ch not in string.punctuation]) #remove punctuation\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    return [lemmatizer.lemmatize(token) for token in tokens if token not in stopwords] \n",
    "    #we have to filter for stopwprds at this stage, otherwise some stopwords get truncated and missed if passed to CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc2f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=tokenize_lemmatize, stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9efa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec = vectorizer.fit_transform(corpus)\n",
    "X_vec_df = pd.DataFrame(X_vec.todense(), columns=vectorizer.get_feature_names_out(), index=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec_df.reset_index().rename(columns={'index': 'Singer'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec_df.reset_index(inplace=True)\n",
    "X_vec_df.rename(columns={'index': 'Singer'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63985c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df83c10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_vec_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812aa1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_vec_df.drop(['Singer'], axis=1)\n",
    "\n",
    "\n",
    "y = X_vec_df['Singer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c7a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644be3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008cdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x, y, random_state=42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41087016",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013928ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db7df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdaf6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lyrics = [\"Cause Iâ€²m not your princess, this ain't a fairytale\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bcc85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vectors = vectorizer.transform(new_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9715028",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vectors.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c62d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(new_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c2ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(new_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
